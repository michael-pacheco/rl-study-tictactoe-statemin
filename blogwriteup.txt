In the field of Reinforcement Learning, there have been many games that have been "beaten" successfully. That is, an agent has been trained to a point where it could play the game perfectly, or if competing, can even defeat opponents. Games ranging from Mario, Flappy Bird, to even chess and checkers have had reinfrocement learning agents trained on them, and very well at that.

One of the concerns that needs to be considered as part of any reinforcement learning problem is the complexity or dimensionality of the problem. This relates directly to how many states and or actions may be required to have stored in memory as information for the agent to use to its advantage. Many of the games that we exist and play are actually fairly complex - there are many states that a single game environment could be in, and additionaly many actions the player(s) could perform in each of those states.

For example, let us consider some of the games that this is exaggerated in: chess and Go. In chess, after the first turns are played by both players, there would already have been 400 possible states the game could have resulted in.
After each of the players have gone a second time, 197,742 different states could have resulted. After the third, over 121 million states could have been played out.

This concern has been the reason why it was thought that some agents could simply not learn to play chess. This was the case until the hero (Deep Q Networks) came to save the day. With this advancement in machine learning, even though there were still many states that could play out, the addition of a neural network in combination with reinforcement learning algorithms lead to a more efficient way for the agent to grasp its environment - mainly through approximations leveraged from the neural network, and using a matrix of pixel values for game states.

For agents, they are more versatile the more information they have about their environment. This statement is backed by the importance of exploration vs exploitation - the agent must be able to explore most of its environment as it can, as a way to discover each of the states and actions that may result in the maximization of its returns.

Tic tac toe is a game featured in R. Sutton's book of reinforcement learning as an assignment. This game is much simpler than chess or checkers, in that there are much less states and actions that exist in the environment. 
Even so, the amount of states or boards of tic tac toe is 255,168.


Although there are a lot of states that can potentially be resulted in, we can actually reduce this number.

If we look at the standard board of tic tac toe, we can see that its shape is a square - four equal straight sides and four right angles. A square, along with other shapes, can be rotated or flipped freely, and will still remain its properties that define it to be a shape.
Applying this to our board, as we rotate the board, the resulting board is actually always valid - it is a board that can be arrived at as a result of two players making precise actions that can lead to that state.
Considering this idea, we are able to analyze some of the actions that can be made by the players and conclude they are realistically equivalent to others, and use this to prune a lot of the potential states.


I also believe it can be argued that although equivalent in theory, each sequence of actions are unique to each of the players, can is influenced by the individuals playing, their relationship, past games played and their outcomes, level of experience of each of the players, and possibly a few more concepts.

Understanding the symmetrical properties of this game, we can argue that these four states:
(0, 1), (1, 0), (1, 2), (2, 1)


are equivalent, and additionally, these four states:
(0, 0), (0, 2), (2, 0), (2, 2)


are as well. The only unique action that is possible is the middle of the board (1, 1). However if the player claims the middle of the board, then the same rule applies for the second player - the four actions in either of those two sets of actions are equivalent.
What this means is that if the first or second player's first action is in any of these 8 states, we can choose a default orientation of the board and make sure to flip the board into that orientation.

If we consider the possible first actions for each player, (9 vs 8), we are essentially minimizing or limiting the first player's set of 9 actions to one of three. If the first player chooses to placer their marker in the middle of the board, we can then limit the second player's set of 8 actions to one of two actions. This can also be thought of as dividing the action space by 3 or 12 for each situation respectively - and as a result reducing total possible amount of states by 3 (if the first player chooses to place their marker in [2, 0] or [2, 1]), or 12 (if player one chooses the middle, and we restrict player two's first move to either [2, 0] or [2, 1]).

With a smaller action and state space the agent should be able to complete games quicker, but limiting the agent's actions may have an impact on its ability to maximize its reward. 

With these hypotheses in mind, I was curious of the difference in performance of two agents: one where the actions of the players are restricted to mimic the idea of rotations being performed on the board and equivalent actions, and another that does not do this, by considering all 9 actions for its first move.











__________________________________________________________________________________________________________
When calculating amount of states, actions must be considered. Let us represent the board as a matrix, where indicies [0, 0] are the top left, and [2, 2] the bottom right. In a game of Tic Tac Toe, most of the time the first player is respresented by an X, and the second an O.
If we start in the first state (a blank board), the first player has nine moves or actions they can perform in each of these coordinates:
	[0, 0]
	[0, 1]
	[0, 2]
	[1, 0]
	[1, 1]
	[1, 2]
	[2, 0]
	[2, 1]
	[2, 2]

With access to each of these nine actions, nine different states can result from each of the respective actions performed. After the first player acts, the second player has eight moves under consideration, which will result in eight new states. This pattern continues until nine moves between both players have been made, or until one of the players are victorious.
Considering this pattern, the amount of states can be calculated by:
	1 (first state) +
	( 9 (first player) *
	  8 (second player) *
	  7
	  6
	  .
	  .
	  .
	  1 )
	  
Which comes out as 9! + 1 = 362881. This is a lot of states, however there are actually less; some of these combinations of actions do not actually exist, as they will be terminated in an earlier state as a result of a win. 
To calculate this, we need to consider every state that is a terminal state - one where either player has won a round, and prune the states that result after that state.


We know the least amount of turns the first player can win in is 3 (5 including the second player's actions), and 3 for the second player (6 including the first player's actions).
